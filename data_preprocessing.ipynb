{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66cd15e4",
   "metadata": {},
   "source": [
    "# Hyperspectral Data Preprocessing Pipeline\n",
    "\n",
    "This notebook implements a comprehensive pipeline for hyperspectral data processing including:\n",
    "- Data acquisition and download\n",
    "- Radiometric & atmospheric correction\n",
    "- Geometric corrections and orthorectification\n",
    "- Dimensionality reduction (MNF, PCA)\n",
    "- Spectral analysis and unmixing\n",
    "- Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da313fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import ndimage, linalg\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths and visualization\n",
    "DATA_DIR = Path('./data')\n",
    "OUTPUT_DIR = Path('./outputs')\n",
    "FIGURES_DIR = Path('./figures')\n",
    "\n",
    "for dir_path in [DATA_DIR, OUTPUT_DIR, FIGURES_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28886abd",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAcquisition:\n",
    "    \"\"\"Handle hyperspectral data download and organization\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_usgs_data(product: str, bounds: Tuple[float, float, float, float], \n",
    "                          output_path: Path) -> str:\n",
    "        \"\"\"\n",
    "        Download hyperspectral data from USGS EarthExplorer\n",
    "        \n",
    "        Args:\n",
    "            product: Product ID (e.g., 'LANDSAT_8', 'SENTINEL_2')\n",
    "            bounds: (minx, miny, maxx, maxy) geographic bounds\n",
    "            output_path: Where to save downloaded data\n",
    "            \n",
    "        Note: Requires usgs.gov API key setup\n",
    "        \"\"\"\n",
    "        print(f\"Downloading {product} data for bounds: {bounds}\")\n",
    "        # Implementation would use requests library and USGS API\n",
    "        # For demo: return mock path\n",
    "        return str(output_path / f\"{product}_data.tif\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_metadata(filepath: Path) -> Dict:\n",
    "        \"\"\"Extract and validate metadata from hyperspectral data\"\"\"\n",
    "        metadata = {\n",
    "            'filepath': str(filepath),\n",
    "            'exists': filepath.exists(),\n",
    "        }\n",
    "        \n",
    "        if filepath.exists() and filepath.suffix == '.tif':\n",
    "            try:\n",
    "                with rasterio.open(filepath) as src:\n",
    "                    metadata.update({\n",
    "                        'bands': src.count,\n",
    "                        'width': src.width,\n",
    "                        'height': src.height,\n",
    "                        'dtype': str(src.dtypes[0]),\n",
    "                        'crs': str(src.crs),\n",
    "                        'bounds': src.bounds,\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                metadata['error'] = str(e)\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    @staticmethod\n",
    "    def organize_data(raw_dir: Path, organized_dir: Path) -> None:\n",
    "        \"\"\"Organize raw data into standardized directory structure\"\"\"\n",
    "        structure = {\n",
    "            'raw': organized_dir / 'raw',\n",
    "            'processed': organized_dir / 'processed',\n",
    "            'radiometric': organized_dir / 'processed/radiometric_correction',\n",
    "            'atmospheric': organized_dir / 'processed/atmospheric_correction',\n",
    "            'geometric': organized_dir / 'processed/geometric_correction',\n",
    "            'dimensionality': organized_dir / 'processed/dimensionality_reduction',\n",
    "            'analysis': organized_dir / 'processed/spectral_analysis',\n",
    "        }\n",
    "        \n",
    "        for dir_path in structure.values():\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"Data structure created: {organized_dir}\")\n",
    "\n",
    "# Example usage\n",
    "# acq = DataAcquisition()\n",
    "# metadata = acq.validate_metadata(DATA_DIR / 'sample.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d47c36",
   "metadata": {},
   "source": [
    "## 2. Radiometric Correction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadiometricCorrection:\n",
    "    \"\"\"Digital Number (DN) to Radiance conversion and calibration\"\"\"\n",
    "    \n",
    "    def __init__(self, sensor_gain: np.ndarray, sensor_offset: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize with sensor-specific calibration parameters\n",
    "        \n",
    "        Args:\n",
    "            sensor_gain: Per-band gain (ML) values\n",
    "            sensor_offset: Per-band offset (AL) values\n",
    "        \"\"\"\n",
    "        self.gain = sensor_gain\n",
    "        self.offset = sensor_offset\n",
    "    \n",
    "    def dn_to_radiance(self, dn_image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert Digital Numbers to Radiance\n",
    "        L = ML * DN + AL\n",
    "        \n",
    "        Args:\n",
    "            dn_image: Raw DN image (bands, height, width)\n",
    "            \n",
    "        Returns:\n",
    "            Radiance image\n",
    "        \"\"\"\n",
    "        radiance = np.zeros_like(dn_image, dtype=np.float32)\n",
    "        \n",
    "        for band in range(dn_image.shape[0]):\n",
    "            radiance[band] = self.gain[band] * dn_image[band].astype(np.float32) + self.offset[band]\n",
    "        \n",
    "        return radiance\n",
    "    \n",
    "    def thermal_to_brightness_temp(self, thermal_band: np.ndarray, ml1: float, al1: float,\n",
    "                                   k1: float, k2: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert thermal band to brightness temperature\n",
    "        \n",
    "        Args:\n",
    "            thermal_band: Thermal radiance band\n",
    "            ml1, al1: Thermal band gain/offset\n",
    "            k1, k2: Thermal constants (sensor-specific)\n",
    "            \n",
    "        Returns:\n",
    "            Brightness temperature in Kelvin\n",
    "        \"\"\"\n",
    "        radiance = ml1 * thermal_band.astype(np.float32) + al1\n",
    "        bt = k2 / np.log((k1 / radiance) + 1)\n",
    "        return bt\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_striping(image: np.ndarray, window_size: int = 5) -> np.ndarray:\n",
    "        \"\"\"Remove striping artifacts common in hyperspectral data\"\"\"\n",
    "        corrected = np.zeros_like(image, dtype=np.float32)\n",
    "        \n",
    "        for band in range(image.shape[0]):\n",
    "            band_data = image[band].astype(np.float32)\n",
    "            # Use median filtering to remove stripe noise\n",
    "            corrected[band] = ndimage.median_filter(band_data, size=(window_size, window_size))\n",
    "        \n",
    "        return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baefe6f8",
   "metadata": {},
   "source": [
    "## 3. Atmospheric Correction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtmosphericCorrection:\n",
    "    \"\"\"Atmospheric correction to surface reflectance\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quac_simple(toa_reflectance: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simple QUAC (Quick Atmospheric Correction) implementation\n",
    "        Uses dark object subtraction\n",
    "        \n",
    "        Args:\n",
    "            toa_reflectance: Top-of-atmosphere reflectance (0-1)\n",
    "            \n",
    "        Returns:\n",
    "            Surface reflectance\n",
    "        \"\"\"\n",
    "        surface_ref = toa_reflectance.copy().astype(np.float32)\n",
    "        \n",
    "        for band in range(surface_ref.shape[0]):\n",
    "            # Find dark object (minimum reflectance percentile)\n",
    "            dark_object = np.percentile(surface_ref[band], 0.5)\n",
    "            # Dark object subtraction\n",
    "            surface_ref[band] = (surface_ref[band] - dark_object) / (1 - dark_object)\n",
    "            # Clip to valid range\n",
    "            surface_ref[band] = np.clip(surface_ref[band], 0, 1)\n",
    "        \n",
    "        return surface_ref\n",
    "    \n",
    "    @staticmethod\n",
    "    def flaash_simple(radiance: np.ndarray, solar_zenith: float, \n",
    "                     visibility_km: float = 40) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simplified FLAASH (Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes)\n",
    "        Uses empirical coefficients\n",
    "        \n",
    "        Args:\n",
    "            radiance: Spectral radiance\n",
    "            solar_zenith: Solar zenith angle in degrees\n",
    "            visibility_km: Atmospheric visibility in km\n",
    "            \n",
    "        Returns:\n",
    "            Surface reflectance\n",
    "        \"\"\"\n",
    "        # Simplified FLAASH with standard atmosphere\n",
    "        air_mass = 1 / np.cos(np.radians(solar_zenith))\n",
    "        \n",
    "        # Empirical transmittance based on visibility\n",
    "        tau = np.exp(-visibility_km / 50)  # Simplified\n",
    "        \n",
    "        # Surface reflectance = (π * L) / (E0 * τ * cos(θ))\n",
    "        e0 = 1361  # Solar constant in W/m^2/μm (simplified)\n",
    "        surface_ref = (np.pi * radiance) / (e0 * tau * np.cos(np.radians(solar_zenith)))\n",
    "        \n",
    "        return np.clip(surface_ref, 0, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectral_angle_mapper_correction(spectrum: np.ndarray, \n",
    "                                        reference_spectrum: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Spectral Angle Mapper for spectral similarity\n",
    "        \n",
    "        Args:\n",
    "            spectrum: Test spectrum\n",
    "            reference_spectrum: Reference spectrum\n",
    "            \n",
    "        Returns:\n",
    "            Spectral angle in radians\n",
    "        \"\"\"\n",
    "        # Normalize spectra\n",
    "        norm_test = spectrum / (np.linalg.norm(spectrum) + 1e-8)\n",
    "        norm_ref = reference_spectrum / (np.linalg.norm(reference_spectrum) + 1e-8)\n",
    "        \n",
    "        # Calculate angle\n",
    "        cos_angle = np.dot(norm_test, norm_ref)\n",
    "        cos_angle = np.clip(cos_angle, -1, 1)\n",
    "        angle = np.arccos(cos_angle)\n",
    "        \n",
    "        return angle\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_quality_mask(image: np.ndarray, cloud_mask: np.ndarray, \n",
    "                          water_mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply quality masks to remove clouds and water\"\"\"\n",
    "        masked_image = image.copy()\n",
    "        bad_pixels = cloud_mask | water_mask\n",
    "        \n",
    "        masked_image[:, bad_pixels] = np.nan\n",
    "        \n",
    "        return masked_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa1536",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction Module (MNF/PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionalityReduction:\n",
    "    \"\"\"MNF and PCA transformation for hyperspectral data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_mnf(image: np.ndarray, n_components: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Minimum Noise Fraction (MNF) transformation\n",
    "        First applies noise-whitening, then PCA\n",
    "        \n",
    "        Args:\n",
    "            image: Input hyperspectral image (bands, height, width)\n",
    "            n_components: Number of MNF components to extract\n",
    "            \n",
    "        Returns:\n",
    "            Transformed image, eigenvalues\n",
    "        \"\"\"\n",
    "        bands, height, width = image.shape\n",
    "        \n",
    "        # Reshape to (n_pixels, n_bands)\n",
    "        data = image.reshape(bands, -1).T\n",
    "        \n",
    "        # Estimate noise covariance (using local differences)\n",
    "        noise_cov = np.zeros((bands, bands))\n",
    "        for i in range(height - 1):\n",
    "            for j in range(width - 1):\n",
    "                diff = image[:, i, j] - image[:, i+1, j]\n",
    "                noise_cov += np.outer(diff, diff)\n",
    "        noise_cov /= ((height - 1) * (width - 1))\n",
    "        \n",
    "        # Signal covariance\n",
    "        signal_cov = np.cov(data.T)\n",
    "        \n",
    "        # Eigendecomposition of noise-whitened covariance\n",
    "        try:\n",
    "            L_inv = linalg.cholesky(noise_cov, lower=True)\n",
    "            L_inv = linalg.inv(L_inv)\n",
    "            whitened_cov = L_inv @ signal_cov @ L_inv.T\n",
    "            \n",
    "            eigenvalues, eigenvectors = linalg.eigh(whitened_cov)\n",
    "            idx = np.argsort(eigenvalues)[::-1]\n",
    "            \n",
    "            eigenvalues = eigenvalues[idx]\n",
    "            eigenvectors = eigenvectors[:, idx]\n",
    "            \n",
    "            # Select top components\n",
    "            eigenvectors = eigenvectors[:, :n_components]\n",
    "            \n",
    "            # Transform data\n",
    "            transformed = data @ L_inv.T @ eigenvectors\n",
    "            \n",
    "        except linalg.LinAlgError:\n",
    "            # Fall back to PCA if MNF fails\n",
    "            return DimensionalityReduction.compute_pca(image, n_components)\n",
    "        \n",
    "        return transformed.reshape(height, width, n_components), eigenvalues[:n_components]\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_pca(image: np.ndarray, n_components: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Principal Component Analysis (PCA) transformation\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (bands, height, width)\n",
    "            n_components: Number of PCA components\n",
    "            \n",
    "        Returns:\n",
    "            Transformed image, explained variance ratio\n",
    "        \"\"\"\n",
    "        bands, height, width = image.shape\n",
    "        \n",
    "        # Reshape to (n_pixels, n_bands)\n",
    "        data = image.reshape(bands, -1).T\n",
    "        \n",
    "        # Standardize\n",
    "        mean = data.mean(axis=0)\n",
    "        std = data.std(axis=0) + 1e-8\n",
    "        data_scaled = (data - mean) / std\n",
    "        \n",
    "        # Covariance matrix\n",
    "        cov = np.cov(data_scaled.T)\n",
    "        \n",
    "        # Eigendecomposition\n",
    "        eigenvalues, eigenvectors = linalg.eigh(cov)\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        \n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # Select top components\n",
    "        eigenvectors = eigenvectors[:, :n_components]\n",
    "        \n",
    "        # Transform\n",
    "        transformed = data_scaled @ eigenvectors\n",
    "        explained_variance_ratio = eigenvalues[:n_components] / eigenvalues.sum()\n",
    "        \n",
    "        return transformed.reshape(height, width, n_components), explained_variance_ratio\n",
    "    \n",
    "    @staticmethod\n",
    "    def select_bands(image: np.ndarray, method: str = 'entropy', \n",
    "                    n_bands: int = 10) -> Tuple[np.ndarray, List[int]]:\n",
    "        \"\"\"\n",
    "        Select informative bands based on various criteria\n",
    "        \n",
    "        Args:\n",
    "            image: Hyperspectral image (bands, height, width)\n",
    "            method: 'entropy', 'variance', or 'correlation'\n",
    "            n_bands: Number of bands to select\n",
    "            \n",
    "        Returns:\n",
    "            Selected bands image, indices of selected bands\n",
    "        \"\"\"\n",
    "        bands, height, width = image.shape\n",
    "        scores = np.zeros(bands)\n",
    "        \n",
    "        if method == 'entropy':\n",
    "            # Entropy-based selection\n",
    "            for b in range(bands):\n",
    "                hist, _ = np.histogram(image[b].ravel(), bins=256)\n",
    "                hist = hist / hist.sum()\n",
    "                scores[b] = -np.sum(hist[hist > 0] * np.log2(hist[hist > 0] + 1e-8))\n",
    "        \n",
    "        elif method == 'variance':\n",
    "            # Variance-based selection\n",
    "            for b in range(bands):\n",
    "                scores[b] = np.var(image[b])\n",
    "        \n",
    "        elif method == 'correlation':\n",
    "            # Decorrelation-based selection\n",
    "            data = image.reshape(bands, -1)\n",
    "            corr = np.corrcoef(data)\n",
    "            scores = np.sum(np.abs(corr), axis=1)\n",
    "        \n",
    "        # Select top bands\n",
    "        selected_indices = np.argsort(scores)[::-1][:n_bands]\n",
    "        selected_indices = np.sort(selected_indices)\n",
    "        \n",
    "        selected_image = image[selected_indices]\n",
    "        \n",
    "        return selected_image, selected_indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4db36d",
   "metadata": {},
   "source": [
    "## 5. Spectral Unmixing & Analysis Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fe63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralUnmixing:\n",
    "    \"\"\"Linear and nonlinear spectral unmixing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_spectral_unmixing(image: np.ndarray, \n",
    "                                 endmembers: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Linear Spectral Unmixing: X = M * A + Noise\n",
    "        \n",
    "        Args:\n",
    "            image: Hyperspectral image (bands, height, width)\n",
    "            endmembers: Endmember signatures (bands, n_endmembers)\n",
    "            \n",
    "        Returns:\n",
    "            Abundance maps (height, width, n_endmembers)\n",
    "        \"\"\"\n",
    "        bands, height, width = image.shape\n",
    "        n_endmembers = endmembers.shape[1]\n",
    "        \n",
    "        data = image.reshape(bands, -1).T  # (n_pixels, bands)\n",
    "        \n",
    "        # Non-negative least squares for each pixel\n",
    "        abundances = np.zeros((height * width, n_endmembers))\n",
    "        \n",
    "        for pixel in range(height * width):\n",
    "            # Constrained least squares (non-negative)\n",
    "            try:\n",
    "                # Using simple least squares (for full solution use scipy.optimize.nnls)\n",
    "                abundance, _ = np.linalg.lstsq(endmembers, data[pixel], rcond=None)\n",
    "                abundances[pixel] = np.maximum(abundance, 0)\n",
    "                # Normalize to sum to 1\n",
    "                abundances[pixel] /= (abundances[pixel].sum() + 1e-8)\n",
    "            except:\n",
    "                abundances[pixel] = 1.0 / n_endmembers\n",
    "        \n",
    "        return abundances.reshape(height, width, n_endmembers)\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_endmembers_ppi(image: np.ndarray, n_endmembers: int = 5,\n",
    "                               iterations: int = 100) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pixel Purity Index (PPI) for endmember extraction\n",
    "        \n",
    "        Args:\n",
    "            image: Hyperspectral image (bands, height, width)\n",
    "            n_endmembers: Number of endmembers to extract\n",
    "            iterations: Number of random projection iterations\n",
    "            \n",
    "        Returns:\n",
    "            Endmember signatures (bands, n_endmembers)\n",
    "        \"\"\"\n",
    "        bands, height, width = image.shape\n",
    "        data = image.reshape(bands, -1).T  # (n_pixels, bands)\n",
    "        \n",
    "        purity_scores = np.zeros(height * width)\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            # Random projection vector\n",
    "            proj = np.random.randn(bands)\n",
    "            proj /= np.linalg.norm(proj)\n",
    "            \n",
    "            # Project data\n",
    "            projection = data @ proj\n",
    "            \n",
    "            # Find extreme pixels (min/max)\n",
    "            min_idx = np.argmin(projection)\n",
    "            max_idx = np.argmax(projection)\n",
    "            \n",
    "            purity_scores[min_idx] += 1\n",
    "            purity_scores[max_idx] += 1\n",
    "        \n",
    "        # Select endmembers from highest PPI pixels\n",
    "        top_indices = np.argsort(purity_scores)[-n_endmembers:]\n",
    "        endmembers = data[top_indices].T  # (bands, n_endmembers)\n",
    "        \n",
    "        return endmembers\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectral_indices(image: np.ndarray, wavelengths: Optional[np.ndarray] = None) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate common spectral indices\n",
    "        \n",
    "        Args:\n",
    "            image: Hyperspectral image (bands, height, width)\n",
    "            wavelengths: Wavelengths for each band (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of calculated indices\n",
    "        \"\"\"\n",
    "        bands, height, width = image.shape\n",
    "        indices = {}\n",
    "        \n",
    "        # Simplified indices using band positions\n",
    "        # Assuming rough band ordering: VIS, NIR, SWIR\n",
    "        \n",
    "        if bands >= 3:\n",
    "            # Simple NDVI-like index\n",
    "            red = image[bands//3]\n",
    "            nir = image[2*bands//3]\n",
    "            indices['NDVI'] = (nir - red) / (nir + red + 1e-8)\n",
    "        \n",
    "        if bands >= 4:\n",
    "            # Simple NIR/SWIR ratio\n",
    "            nir = image[2*bands//3]\n",
    "            swir = image[3*bands//4]\n",
    "            indices['NDBI'] = (swir - nir) / (swir + nir + 1e-8)\n",
    "        \n",
    "        # Mean spectral signature\n",
    "        indices['mean_spectrum'] = np.mean(image, axis=(1, 2))\n",
    "        \n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c8fe2",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48810ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetection:\n",
    "    \"\"\"Spectral and spatial anomaly detection\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def rx_detector(image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reed-Xiaoli (RX) Anomaly Detector\n",
    "        Uses Mahalanobis distance from background statistics\n",
    "        \n",
    "        Args:\n",
    "            image: Hyperspectral image (bands, height, width)\n",
    "            \n",
    "        Returns:\n",
    "            RX score map (height, width)\n",
    "        \"\"\"\n",
    "        bands, height, width = image.shape\n",
    "        data = image.reshape(bands, -1).T  # (n_pixels, bands)\n",
    "        \n",
    "        # Estimate background mean and covariance\n",
    "        mean = np.mean(data, axis=0)\n",
    "        cov = np.cov(data.T) + np.eye(bands) * 1e-3  # Add regularization\n",
    "        \n",
    "        # Inverse covariance\n",
    "        try:\n",
    "            inv_cov = np.linalg.inv(cov)\n",
    "        except np.linalg.LinAlgError:\n",
    "            inv_cov = np.linalg.pinv(cov)\n",
    "        \n",
    "        # RX score for each pixel\n",
    "        rx_scores = np.zeros(height * width)\n",
    "        \n",
    "        for pixel in range(height * width):\n",
    "            diff = data[pixel] - mean\n",
    "            rx_scores[pixel] = diff @ inv_cov @ diff.T\n",
    "        \n",
    "        return rx_scores.reshape(height, width)\n",
    "    \n",
    "    @staticmethod\n",
    "    def local_rx_detector(image: np.ndarray, window_size: int = 15) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Local RX detector using sliding window\n",
    "        Better for detecting anomalies in heterogeneous scenes\n",
    "        \n",
    "        Args:\n",
    "            image: Hyperspectral image (bands, height, width)\n",
    "            window_size: Size of local window\n",
    "            \n",
    "        Returns:\n",
    "            Local RX score map (height, width)\n",
    "        \"\"\"\n",
    "        bands, height, width = image.shape\n",
    "        local_rx = np.zeros((height, width))\n",
    "        pad = window_size // 2\n",
    "        \n",
    "        for i in range(pad, height - pad):\n",
    "            for j in range(pad, width - pad):\n",
    "                # Extract local window\n",
    "                window = image[:, i-pad:i+pad+1, j-pad:j+pad+1]\n",
    "                window_data = window.reshape(bands, -1).T\n",
    "                \n",
    "                # Local statistics\n",
    "                mean = np.mean(window_data, axis=0)\n",
    "                cov = np.cov(window_data.T) + np.eye(bands) * 1e-3\n",
    "                \n",
    "                try:\n",
    "                    inv_cov = np.linalg.inv(cov)\n",
    "                except np.linalg.LinAlgError:\n",
    "                    inv_cov = np.linalg.pinv(cov)\n",
    "                \n",
    "                # RX score for center pixel\n",
    "                center_spectrum = image[:, i, j]\n",
    "                diff = center_spectrum - mean\n",
    "                local_rx[i, j] = diff @ inv_cov @ diff.T\n",
    "        \n",
    "        return local_rx\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectral_angle_anomaly(image: np.ndarray, \n",
    "                              reference_spectrum: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Spectral Angle Mapper-based anomaly detection\n",
    "        \n",
    "        Args:\n",
    "            image: Hyperspectral image (bands, height, width)\n",
    "            reference_spectrum: Reference spectrum (default: mean spectrum)\n",
    "            \n",
    "        Returns:\n",
    "            Spectral angle map in radians (height, width)\n",
    "        \"\"\"\n",
    "        bands, height, width = image.shape\n",
    "        \n",
    "        if reference_spectrum is None:\n",
    "            reference_spectrum = np.mean(image, axis=(1, 2))\n",
    "        \n",
    "        # Normalize reference\n",
    "        ref_norm = reference_spectrum / (np.linalg.norm(reference_spectrum) + 1e-8)\n",
    "        \n",
    "        angle_map = np.zeros((height, width))\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                spectrum = image[:, i, j]\n",
    "                spec_norm = spectrum / (np.linalg.norm(spectrum) + 1e-8)\n",
    "                \n",
    "                cos_angle = np.dot(spec_norm, ref_norm)\n",
    "                cos_angle = np.clip(cos_angle, -1, 1)\n",
    "                angle_map[i, j] = np.arccos(cos_angle)\n",
    "        \n",
    "        return angle_map\n",
    "    \n",
    "    @staticmethod\n",
    "    def cluster_anomaly_detection(image: np.ndarray, n_clusters: int = 3) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Clustering-based anomaly detection\n",
    "        \n",
    "        Args:\n",
    "            image: Hyperspectral image (bands, height, width)\n",
    "            n_clusters: Number of clusters\n",
    "            \n",
    "        Returns:\n",
    "            Cluster label map (height, width)\n",
    "        \"\"\"\n",
    "        bands, height, width = image.shape\n",
    "        data = image.reshape(bands, -1).T\n",
    "        \n",
    "        # Simple K-means clustering\n",
    "        # Initialize random centroids\n",
    "        indices = np.random.choice(data.shape[0], n_clusters, replace=False)\n",
    "        centroids = data[indices]\n",
    "        \n",
    "        for iteration in range(10):  # 10 iterations\n",
    "            # Assign clusters\n",
    "            distances = np.zeros((data.shape[0], n_clusters))\n",
    "            for k in range(n_clusters):\n",
    "                distances[:, k] = np.linalg.norm(data - centroids[k], axis=1)\n",
    "            \n",
    "            labels = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Update centroids\n",
    "            for k in range(n_clusters):\n",
    "                centroids[k] = data[labels == k].mean(axis=0)\n",
    "        \n",
    "        return labels.reshape(height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0282b6d",
   "metadata": {},
   "source": [
    "## 7. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b674570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperspectralUtils:\n",
    "    \"\"\"Utility functions for visualization and I/O\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_rgb_composite(image: np.ndarray, r_band: int, g_band: int, \n",
    "                            b_band: int, stretch: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create RGB composite from hyperspectral image\n",
    "        \n",
    "        Args:\n",
    "            image: Hyperspectral image (bands, height, width)\n",
    "            r_band, g_band, b_band: Band indices for R, G, B\n",
    "            stretch: Perform contrast stretching\n",
    "            \n",
    "        Returns:\n",
    "            RGB composite (height, width, 3)\n",
    "        \"\"\"\n",
    "        composite = np.zeros((image.shape[1], image.shape[2], 3), dtype=np.float32)\n",
    "        composite[:, :, 0] = image[r_band]\n",
    "        composite[:, :, 1] = image[g_band]\n",
    "        composite[:, :, 2] = image[b_band]\n",
    "        \n",
    "        if stretch:\n",
    "            # 2% linear stretch\n",
    "            for i in range(3):\n",
    "                vmin, vmax = np.percentile(composite[:, :, i], (2, 98))\n",
    "                composite[:, :, i] = (composite[:, :, i] - vmin) / (vmax - vmin + 1e-8)\n",
    "        \n",
    "        composite = np.clip(composite, 0, 1)\n",
    "        return composite\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_geotiff(array: np.ndarray, filepath: Path, \n",
    "                    profile: Dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Save array as GeoTIFF with geospatial metadata\n",
    "        \n",
    "        Args:\n",
    "            array: Data array\n",
    "            filepath: Output file path\n",
    "            profile: Rasterio profile with CRS, transform, etc.\n",
    "        \"\"\"\n",
    "        if array.ndim == 2:\n",
    "            height, width = array.shape\n",
    "            count = 1\n",
    "        else:\n",
    "            count, height, width = array.shape\n",
    "        \n",
    "        if profile is None:\n",
    "            profile = {\n",
    "                'driver': 'GTiff',\n",
    "                'height': height,\n",
    "                'width': width,\n",
    "                'count': count,\n",
    "                'dtype': array.dtype,\n",
    "                'crs': 'EPSG:4326',\n",
    "                'transform': rasterio.Affine(1.0, 0.0, 0.0, 0.0, -1.0, 0.0),\n",
    "            }\n",
    "        \n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with rasterio.open(filepath, 'w', **profile) as dst:\n",
    "            if array.ndim == 2:\n",
    "                dst.write(array, 1)\n",
    "            else:\n",
    "                for i in range(count):\n",
    "                    dst.write(array[i], i + 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_spectrum(spectrum: np.ndarray, title: str = 'Spectrum',\n",
    "                     wavelengths: Optional[np.ndarray] = None) -> None:\n",
    "        \"\"\"Plot a spectral signature\"\"\"\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        if wavelengths is not None:\n",
    "            plt.plot(wavelengths, spectrum, linewidth=2)\n",
    "            plt.xlabel('Wavelength (μm)')\n",
    "        else:\n",
    "            plt.plot(spectrum, linewidth=2)\n",
    "            plt.xlabel('Band Number')\n",
    "        \n",
    "        plt.ylabel('Reflectance')\n",
    "        plt.title(title)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_heatmap(array_2d: np.ndarray, title: str = 'Heatmap',\n",
    "                    cmap: str = 'viridis') -> None:\n",
    "        \"\"\"Plot 2D array as heatmap\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        im = plt.imshow(array_2d, cmap=cmap, aspect='auto')\n",
    "        plt.colorbar(im, label='Value')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Pixel X')\n",
    "        plt.ylabel('Pixel Y')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_statistics(image: np.ndarray) -> Dict:\n",
    "        \"\"\"Calculate image statistics\"\"\"\n",
    "        stats = {\n",
    "            'mean_spectrum': np.mean(image, axis=(1, 2)),\n",
    "            'std_spectrum': np.std(image, axis=(1, 2)),\n",
    "            'min': np.min(image),\n",
    "            'max': np.max(image),\n",
    "            'global_mean': np.mean(image),\n",
    "            'global_std': np.std(image),\n",
    "        }\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4170f5",
   "metadata": {},
   "source": [
    "## 8. Example: Complete Processing Pipeline\n",
    "\n",
    "### Generate Synthetic Hyperspectral Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic hyperspectral image for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "# Image dimensions\n",
    "n_bands = 50  # 50 spectral bands\n",
    "height, width = 100, 100\n",
    "\n",
    "# Generate base image with different materials (endmembers)\n",
    "synthetic_image = np.zeros((n_bands, height, width), dtype=np.float32)\n",
    "\n",
    "# Endmember 1: Low reflectance material (e.g., water/soil)\n",
    "endmem_1 = np.exp(-np.linspace(0, 3, n_bands)) * 0.3\n",
    "# Endmember 2: Mid reflectance material\n",
    "endmem_2 = 0.5 + 0.1 * np.sin(np.linspace(0, 4*np.pi, n_bands))\n",
    "# Endmember 3: High reflectance material (e.g., rock/mineral)\n",
    "endmem_3 = 0.8 + 0.1 * np.cos(np.linspace(0, 2*np.pi, n_bands))\n",
    "\n",
    "# Mix endmembers spatially\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        # Create spatial mixture\n",
    "        if i < height//3:\n",
    "            synthetic_image[:, i, j] = endmem_1 + 0.05 * np.random.randn(n_bands)\n",
    "        elif i < 2*height//3:\n",
    "            synthetic_image[:, i, j] = endmem_2 + 0.05 * np.random.randn(n_bands)\n",
    "        else:\n",
    "            synthetic_image[:, i, j] = endmem_3 + 0.05 * np.random.randn(n_bands)\n",
    "\n",
    "# Add small anomalies (e.g., mineral deposits)\n",
    "synthetic_image[:, 20:30, 20:30] = 0.2 + 0.05 * np.random.randn(n_bands, 10, 10)\n",
    "\n",
    "# Clip to valid range\n",
    "synthetic_image = np.clip(synthetic_image, 0, 1)\n",
    "\n",
    "print(f\"Generated synthetic image shape: {synthetic_image.shape}\")\n",
    "print(f\"Value range: [{synthetic_image.min():.3f}, {synthetic_image.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0023e4e",
   "metadata": {},
   "source": [
    "### Step 1: Radiometric Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310228cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize radiometric correction with sensor parameters\n",
    "sensor_gain = np.ones(n_bands) * 0.0003  # ML value\n",
    "sensor_offset = np.ones(n_bands) * 0.1   # AL value\n",
    "\n",
    "rad_corrector = RadiometricCorrection(sensor_gain, sensor_offset)\n",
    "\n",
    "# Simulate DN values (convert reflectance to DN for demo)\n",
    "dn_image = (synthetic_image / 0.0003 - 0.1).astype(np.uint16)\n",
    "dn_image = np.clip(dn_image, 0, 10000)\n",
    "\n",
    "# Convert DN to radiance\n",
    "radiance = rad_corrector.dn_to_radiance(dn_image)\n",
    "\n",
    "# Remove striping artifacts\n",
    "radiance_corrected = rad_corrector.remove_striping(radiance, window_size=3)\n",
    "\n",
    "print(f\"Radiometric correction completed\")\n",
    "print(f\"Radiance range: [{radiance_corrected.min():.3f}, {radiance_corrected.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccedae",
   "metadata": {},
   "source": [
    "### Step 2: Atmospheric Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca323b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to TOA reflectance\n",
    "toa_reflectance = radiance_corrected / radiance_corrected.max()\n",
    "\n",
    "# Apply QUAC atmospheric correction\n",
    "atm_corrector = AtmosphericCorrection()\n",
    "surface_reflectance = atm_corrector.quac_simple(toa_reflectance)\n",
    "\n",
    "print(f\"Atmospheric correction (QUAC) completed\")\n",
    "print(f\"Surface reflectance range: [{surface_reflectance.min():.3f}, {surface_reflectance.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15939ba7",
   "metadata": {},
   "source": [
    "### Step 3: Dimensionality Reduction (PCA & MNF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "dimred = DimensionalityReduction()\n",
    "pca_result, pca_variance = dimred.compute_pca(surface_reflectance, n_components=10)\n",
    "\n",
    "print(f\"PCA completed: {pca_result.shape}\")\n",
    "print(f\"Cumulative explained variance: {pca_variance.cumsum()[-1]:.3f}\")\n",
    "\n",
    "# Try MNF (may produce warnings on synthetic data)\n",
    "try:\n",
    "    mnf_result, mnf_eigenvals = dimred.compute_mnf(surface_reflectance, n_components=10)\n",
    "    print(f\"MNF completed: {mnf_result.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"MNF computation note: {str(e)}\")\n",
    "    mnf_result = pca_result\n",
    "\n",
    "# Select most informative bands\n",
    "selected_image, selected_bands = dimred.select_bands(surface_reflectance, \n",
    "                                                     method='variance', n_bands=10)\n",
    "print(f\"Selected {len(selected_bands)} most informative bands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a16ebb3",
   "metadata": {},
   "source": [
    "### Step 4: Spectral Unmixing & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract endmembers using PPI\n",
    "unmixing = SpectralUnmixing()\n",
    "endmembers = unmixing.extract_endmembers_ppi(surface_reflectance, n_endmembers=3)\n",
    "\n",
    "print(f\"Extracted {endmembers.shape[1]} endmembers\")\n",
    "\n",
    "# Perform spectral unmixing\n",
    "abundances = unmixing.linear_spectral_unmixing(surface_reflectance, endmembers)\n",
    "\n",
    "print(f\"Abundance maps generated: {abundances.shape}\")\n",
    "print(f\"Abundance sum per pixel (should be ~1.0):\", abundances.sum(axis=2).mean())\n",
    "\n",
    "# Calculate spectral indices\n",
    "indices = unmixing.spectral_indices(surface_reflectance)\n",
    "\n",
    "print(f\"Computed spectral indices:\")\n",
    "for idx_name, idx_data in indices.items():\n",
    "    if isinstance(idx_data, np.ndarray) and idx_data.ndim == 2:\n",
    "        print(f\"  {idx_name}: range [{idx_data.min():.3f}, {idx_data.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa151a67",
   "metadata": {},
   "source": [
    "### Step 5: Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply anomaly detectors\n",
    "anomaly = AnomalyDetection()\n",
    "\n",
    "# Global RX detector\n",
    "rx_scores = anomaly.rx_detector(surface_reflectance)\n",
    "print(f\"RX anomaly detection completed: range [{rx_scores.min():.3f}, {rx_scores.max():.3f}]\")\n",
    "\n",
    "# Spectral angle anomaly detection\n",
    "sam_scores = anomaly.spectral_angle_anomaly(surface_reflectance)\n",
    "print(f\"SAM anomaly detection completed: range [{sam_scores.min():.3f}, {sam_scores.max():.3f}]\")\n",
    "\n",
    "# Cluster-based detection\n",
    "labels = anomaly.cluster_anomaly_detection(surface_reflectance, n_clusters=3)\n",
    "print(f\"Cluster detection completed: {len(np.unique(labels))} clusters found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a873ac",
   "metadata": {},
   "source": [
    "### Step 6: Visualization & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = HyperspectralUtils()\n",
    "\n",
    "# 1. Mean spectrum visualization\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Original spectrum\n",
    "ax[0].plot(np.mean(synthetic_image, axis=(1, 2)), linewidth=2, label='Original')\n",
    "ax[0].set_xlabel('Band Number')\n",
    "ax[0].set_ylabel('Reflectance')\n",
    "ax[0].set_title('Mean Spectrum (Original)')\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "# After correction\n",
    "ax[1].plot(np.mean(surface_reflectance, axis=(1, 2)), linewidth=2, label='Corrected', color='orange')\n",
    "ax[1].set_xlabel('Band Number')\n",
    "ax[1].set_ylabel('Reflectance')\n",
    "ax[1].set_title('Mean Spectrum (After Atmospheric Correction)')\n",
    "ax[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'mean_spectra.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Mean spectra visualization saved\")\n",
    "\n",
    "# 2. RGB composite\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "rgb1 = utils.create_rgb_composite(synthetic_image, 5, 25, 45)\n",
    "rgb2 = utils.create_rgb_composite(surface_reflectance, 5, 25, 45)\n",
    "rgb3 = utils.create_rgb_composite(pca_result, 0, 1, 2)\n",
    "\n",
    "ax[0].imshow(rgb1)\n",
    "ax[0].set_title('RGB Composite (Original)')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(rgb2)\n",
    "ax[1].set_title('RGB Composite (Corrected)')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(np.clip(rgb3, 0, 1))\n",
    "ax[2].set_title('RGB Composite (PCA)')\n",
    "ax[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'rgb_composites.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ RGB composites visualization saved\")\n",
    "\n",
    "# 3. Abundance maps from unmixing\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i in range(3):\n",
    "    im = ax[i].imshow(abundances[:, :, i], cmap='viridis')\n",
    "    ax[i].set_title(f'Endmember {i+1} Abundance')\n",
    "    ax[i].axis('off')\n",
    "    plt.colorbar(im, ax=ax[i], label='Abundance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'abundance_maps.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Abundance maps visualization saved\")\n",
    "\n",
    "# 4. Anomaly detection results\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# RX scores\n",
    "im1 = ax[0].imshow(rx_scores, cmap='hot')\n",
    "ax[0].set_title('RX Anomaly Scores')\n",
    "ax[0].axis('off')\n",
    "plt.colorbar(im1, ax=ax[0], label='RX Score')\n",
    "\n",
    "# SAM scores\n",
    "im2 = ax[1].imshow(sam_scores, cmap='hot')\n",
    "ax[1].set_title('Spectral Angle Map')\n",
    "ax[1].axis('off')\n",
    "plt.colorbar(im2, ax=ax[1], label='Angle (rad)')\n",
    "\n",
    "# Cluster labels\n",
    "im3 = ax[2].imshow(labels, cmap='tab10')\n",
    "ax[2].set_title('Cluster Labels')\n",
    "ax[2].axis('off')\n",
    "plt.colorbar(im3, ax=ax[2], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'anomaly_detection.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Anomaly detection visualization saved\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "stats = utils.calculate_statistics(surface_reflectance)\n",
    "print(\"\\n=== Image Statistics ===\")\n",
    "print(f\"Global Mean: {stats['global_mean']:.4f}\")\n",
    "print(f\"Global Std:  {stats['global_std']:.4f}\")\n",
    "print(f\"Value Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperspectral_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
